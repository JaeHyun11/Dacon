{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install open_clip_torch"
      ],
      "metadata": {
        "id": "jm731C4IFMiR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "hx2K4ctubth2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPw-qXp9BCPL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "\n",
        "MODEL_NAME = \"laion/mscoco_finetuned_CoCa-ViT-L-14-laion2B-s13B-b90k\"\n",
        "\n",
        "# 모델 로드 (가중치는 다운로드하지만 실제 연산은 하지 않음)\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('coca_ViT-L-14', pretrained='mscoco_finetuned_laion2B_s13B_b90k')\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "\n",
        "# image = preprocess(Image.open(\"docs/CLIP.png\")).unsqueeze(0)\n",
        "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
        "\n",
        "# 모델의 모든 파라미터 수를 합산\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"모델: {MODEL_NAME}\")\n",
        "print(f\"총 파라미터 수: {total_params:,}\")\n",
        "\n",
        "if total_params >= 1_000_000_000:\n",
        "    print(f\"{total_params / 1_000_000_000:.2f}B 개의 파라미터\")\n",
        "else:\n",
        "    print(f\"{total_params / 1_000_000:.2f}M 개의 파라미터\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import open_clip\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/Dacon/\"\n"
      ],
      "metadata": {
        "id": "lEfza7s8dkT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = os.path.join(base_path, 'data/open.zip')\n",
        "extract_path = '/content/data'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "print(f\"Extracting {zip_path} to {extract_path}...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "m4bg4J8DWXRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "Yaf6CQN4E6X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, image_transform, image_base_path, is_test=False):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "        self.image_base_path = Path(image_base_path)\n",
        "        self.is_test = is_test\n",
        "        self.answer_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = self.image_base_path / row['img_path'].split('/')[-1]\n",
        "        try:\n",
        "            image = self.image_transform(Image.open(image_path).convert(\"RGB\"))\n",
        "        except FileNotFoundError:\n",
        "            image = torch.zeros((3, 224, 224))\n",
        "\n",
        "        question = row['Question']\n",
        "        options = {'A': row['A'], 'B': row['B'], 'C': row['C'], 'D': row['D']}\n",
        "        text_prompts = [f\"Based on the given image, the answer to the question: {question} is: {options[key]}\" for key in ['A', 'B', 'C', 'D']]\n",
        "        tokenized_texts = self.tokenizer(text_prompts)\n",
        "\n",
        "        if self.is_test:\n",
        "            return row['ID'], image, tokenized_texts\n",
        "        else:\n",
        "            answer_label = self.answer_map[row['answer']]\n",
        "            return image, tokenized_texts, torch.tensor(answer_label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "WatMWNOjE6mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"coca_ViT-L-14\"\n",
        "pretrained = \"laion2b_s13b_b90k\"\n",
        "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n",
        "    model_name,\n",
        "    pretrained=pretrained,\n",
        "    device=device\n",
        ")\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "TPWSCzB2E-iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-6\n",
        "\n",
        "# 압축 해제된 데이터 경로 설정\n",
        "TRAIN_CSV_PATH = os.path.join(extract_path, 'train.csv')\n",
        "TRAIN_IMG_DIR = os.path.join(extract_path, 'train_input_images')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 학습 데이터 로드 및 데이터셋/로더 생성\n",
        "original_train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
        "# 훈련/검증 데이터 분리 (stratify=original_train_df['answer']는 정답 비율을 유지)\n",
        "train_df, val_df = train_test_split(\n",
        "    original_train_df,\n",
        "    test_size=0.2, # 검증\n",
        "    random_state=42,\n",
        "    stratify=original_train_df['answer']\n",
        ")\n",
        "\n",
        "print(f\"훈련 데이터: {len(train_df)}개, 검증 데이터: {len(val_df)}개\")\n",
        "\n",
        "train_dataset = VQADataset(\n",
        "    df=train_df,\n",
        "    tokenizer=tokenizer,\n",
        "    image_transform=preprocess_train,\n",
        "    image_base_path=TRAIN_IMG_DIR\n",
        ")\n",
        "val_dataset = VQADataset(\n",
        "    df=val_df,\n",
        "    tokenizer=tokenizer,\n",
        "    image_transform=preprocess_val, # 데이터 증강 없는 전처리기\n",
        "    image_base_path=TRAIN_IMG_DIR\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 옵티마이저 및 손실 함수\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "loss_fn = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "IVzrGgxBFAm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_accuracy = 0.0\n",
        "best_model_path = \"best_vqa_model.pth\" #\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # 훈련\n",
        "    model.train()\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\")\n",
        "    total_loss = 0.0\n",
        "    for images, texts, labels in progress_bar:\n",
        "        images, texts, labels = images.to(device), texts.to(device), labels.to(device)\n",
        "        num_options = texts.shape[1]\n",
        "        texts = texts.view(-1, texts.shape[-1])\n",
        "        optimizer.zero_grad()\n",
        "        image_features = model.encode_image(images)\n",
        "        text_features = model.encode_text(texts)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features.view(-1, num_options, text_features.shape[-1])\n",
        "        logits = torch.einsum('bd,bcd->bc', image_features, text_features) * model.logit_scale.exp()\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Average Training Loss: {avg_loss:.4f}\")\n",
        "# 검증\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, texts, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Validation]\"):\n",
        "            images, texts, labels = images.to(device), texts.to(device), labels.to(device)\n",
        "            num_options = texts.shape[1]\n",
        "            texts = texts.view(-1, texts.shape[-1])\n",
        "\n",
        "            image_features = model.encode_image(images)\n",
        "            text_features = model.encode_text(texts)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            text_features = text_features.view(-1, num_options, text_features.shape[-1])\n",
        "            logits = torch.einsum('bd,bcd->bc', image_features, text_features) * model.logit_scale.exp()\n",
        "\n",
        "            # 정확도 계산\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_total\n",
        "    print(f\"Epoch {epoch+1} Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # 최고 성능 모델 저장\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"🎉 New best model saved with accuracy: {best_val_accuracy:.4f} at {best_model_path}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "jL74vIpKWWhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    total_loss = 0.0\n",
        "    for images, texts, labels in progress_bar:\n",
        "        images = images.to(device)\n",
        "        num_options = texts.shape[1]\n",
        "        texts = texts.view(-1, texts.shape[-1]).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        image_features = model.encode_image(images)\n",
        "        text_features = model.encode_text(texts)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        text_features = text_features.view(-1, num_options, text_features.shape[-1])\n",
        "        logits = torch.einsum('bd,bcd->bc', image_features, text_features) * model.logit_scale.exp()\n",
        "\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "k071HPocFEfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_CSV_PATH = os.path.join(extract_path, 'test.csv')\n",
        "TEST_IMG_DIR = os.path.join(extract_path, 'test_input_images')\n",
        "SUBMISSION_PATH = os.path.join('/content/data', 'submission1.csv')\n",
        "test_df = pd.read_csv(TEST_CSV_PATH)\n",
        "test_dataset = VQADataset(\n",
        "    df=test_df,\n",
        "    tokenizer=tokenizer,\n",
        "    image_transform=preprocess_val,\n",
        "    image_base_path=TEST_IMG_DIR,\n",
        "    is_test=True\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "qmHlzxfPHN9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "ids = []\n",
        "idx_to_answer = {0: 'A', 1: 'B', 2: 'C', 3: 'D'}\n",
        "\n",
        "# 최고 성능 모델의 가중치 로드\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print(f\"Loaded best model from {best_model_path}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_ids, images, texts in tqdm(test_loader, desc=\"Predicting\"):\n",
        "        images = images.to(device)\n",
        "        num_options = texts.shape[1]\n",
        "        texts = texts.view(-1, texts.shape[-1]).to(device)\n",
        "\n",
        "        image_features = model.encode_image(images)\n",
        "        text_features = model.encode_text(texts)\n",
        "\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        text_features = text_features.view(-1, num_options, text_features.shape[-1])\n",
        "        logits = torch.einsum('bd,bcd->bc', image_features, text_features) * model.logit_scale.exp()\n",
        "\n",
        "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        for p in preds:\n",
        "            predictions.append(idx_to_answer[p])\n",
        "        ids.extend(batch_ids)\n",
        "\n",
        "submission_df = pd.DataFrame({'ID': ids, 'answer': predictions})\n",
        "\n",
        "sample_submission_path = os.path.join(extract_path, 'sample_submission.csv')\n",
        "if os.path.exists(sample_submission_path):\n",
        "    sample_df = pd.read_csv(sample_submission_path)\n",
        "    submission_df = submission_df.set_index('ID').loc[sample_df['ID']].reset_index()\n",
        "\n",
        "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
        "\n",
        "print(f\"Inference complete. Submission file saved to {SUBMISSION_PATH}\")"
      ],
      "metadata": {
        "id": "BKTWPFykHQZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}